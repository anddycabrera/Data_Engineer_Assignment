{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6a0e0c",
   "metadata": {},
   "source": [
    "# Data Engineer Assignment: Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc557ef",
   "metadata": {},
   "source": [
    "This take-home assignment evaluates your skills using spark as well as AWS services like S3. You DO NOT need an AWS account since all services will be mock-up locally using moto library so you will always work locally with a docker container and without incurring expenses. By running a docker container you will have all you need to do the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52740436",
   "metadata": {},
   "source": [
    "## Assignment Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fc8e6",
   "metadata": {},
   "source": [
    "You are expected to fill in functions that would complete this assignment. All of the necessary helper code is included in this notebook. Any cell that does not require you to submit code cannot be modified. For example: Assert statement unit test cells. Cells that have \"**# YOUR CODE HERE**\" are the ONLY ones you will need to alter.\n",
    "DO NOT change the names of functions. The assignment contains **3 tasks** that you have to complete. We estimate that the time to complete this task will be between **30 minutes** to **1 hour** depending on your experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed070cb2",
   "metadata": {},
   "source": [
    "Our recommendation is that you first jump to answer cells (search for \"YOUR CODE HERE\") so that you have an idea of what you need to do. After doing that read the notebook as a whole so that you understand the data and the context. You need to decide for yourself how much of the context is sufficient to come up with the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872a2e92",
   "metadata": {},
   "source": [
    "Note: During the assignment when you run some cells you will notice an output with some messages. This is NOT an error, this is the result of running the cell. You can distinguish the error from the output because the outputs use to have a pink background and the error white background."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdf8f9",
   "metadata": {},
   "source": [
    "## Deliverable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ccf74",
   "metadata": {},
   "source": [
    "When you complete the assignment you need to save the Jupyter notebook with your answers and send it to us. If you cannot complete all the tasks you can send us the Jupyter notebook in any way. We will evaluate you with the answers you provided even if you complete only one part of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c47967",
   "metadata": {},
   "source": [
    "### Requirements And How to Start This Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98307e8b",
   "metadata": {},
   "source": [
    "You will need Docker installed and running. [Docker Website](https://www.docker.com/).\n",
    "- Create a folder and place the files **Dockerfile** and **Data_Engineer_Assignment.ipynb** there.\n",
    "- Open the terminal and go(navigate) to the folder location where you saved the files.\n",
    "- In the terminal run this command to build the docker image. This can take up to 10 minutes to complete: `docker build -t spark_assignment:1.0. .`\n",
    "- When docker finishes building the image, run this command in your terminal: `docker run -it --init -p 8888:8888 --name spark spark_assignment:1.0.`\n",
    "- Then open this link to open Jupyter Notebook and start the assignment. Follow the instructions in the Jupiter Notebook: [http://127.0.0.1:8888/tree](http://127.0.0.1:8888/tree)\n",
    "- You are all set. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ce704",
   "metadata": {},
   "source": [
    "## Set AWS Credentials To Work Locally\n",
    "These are fake AWS credentials to work locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd015aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'testing'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'testing'\n",
    "os.environ['AWS_SECURITY_TOKEN'] = 'testing'\n",
    "os.environ['AWS_SESSION_TOKEN'] = 'testing'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a21b24",
   "metadata": {},
   "source": [
    "## Create a Spark Context and Import Dependencies \n",
    "Now we initialize the spark session with the necessary configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import subprocess\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "S3_MOCK_ENDPOINT = \"http://127.0.0.1:5000\"\n",
    " \n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "sc.setLogLevel('OFF')\n",
    "\n",
    "sc.setSystemProperty(\"fs.s3a.bucket.nightly.aws.credentials.provider\", \"dynamic\")\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"mock\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"mock\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", S3_MOCK_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fd47c4",
   "metadata": {},
   "source": [
    "## Set Up Moto Server To Access AWS Locally\n",
    "We start the moto server for AWS s3 resource locally. Don't run it twice or you will get an error that the server is running already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup moto server\n",
    "process = subprocess.Popen(\n",
    "    \"moto_server\", stdout=subprocess.PIPE,\n",
    "    shell=True, preexec_fn=os.setsid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d479f",
   "metadata": {},
   "source": [
    "## Set AWS Services Variables \n",
    "Set up AWS variables for region and bucket name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732767a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-east-1\"\n",
    "BUCKET_NAME = \"assignment-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcfe01",
   "metadata": {},
   "source": [
    "## Create AWS S3 Bucket\n",
    "Running the below cell you will create an s3 bucket which you will use during the assignment. The name of the bucket is stored in the variable 'BUCKET_NAME' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115e10b-b95b-4a06-875a-b4d504f1813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create s3 connection, bucket and s3 url's\n",
    "s3_conn = boto3.resource('s3', region_name=AWS_REGION, endpoint_url=S3_MOCK_ENDPOINT)  \n",
    "\n",
    "# create s3 bucket \n",
    "s3_conn.create_bucket(Bucket=BUCKET_NAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f07e7",
   "metadata": {},
   "source": [
    "## Set a Source and Destination key \n",
    "The below cell contains the source and destination paths that you will use later to read and write into the s3 bucket. Please note that the key is partitioned by date with the format yyyy/mm/dd. Working with this partition will be important during the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821409e-3df6-4c71-9708-15c663ed4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_source = [\n",
    "            \"s3a://{}/{}\".format(BUCKET_NAME, \"data/raw-zone/domain/2022/03/23/\"), \n",
    "            \"s3a://{}/{}\".format(BUCKET_NAME, \"data/raw-zone/domain/2022/03/25/\")\n",
    "            ]         \n",
    "s3_destination = \"s3a://{}/{}\".format(BUCKET_NAME, \"data/raw-zone/datalake/\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d32fb",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "Now we create a data frame with some data that you will use later to be processed during the assignment. The data is compose by the \"Op\", \"sys_change_timestamp\", \"id\", \"name\", \"created_at\", \"updated_at\", \"sys_data_source\" columns. The meaning of this columns is not important for the assignment so don't worry to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd0ccc-3896-4419-ba69-cdd12fccacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create source dataframe and write the dataframe as csv to s3\n",
    "values =   [(\"I\",\n",
    "            \"2022-03-24 21:00:24.803897\",\n",
    "            \"d608fbdb-e975-4b80-89ff-a52c8c4ccca4\",\n",
    "            \"Frederick Melendez\",\n",
    "            \"2022-03-24 21:00:24.801188\",\n",
    "            \"2022-03-24 21:00:24.801194\",\n",
    "            \"domain\"), \n",
    "          \n",
    "            (\"I\",\n",
    "             \"2022-03-24 21:00:24.8038654\",\n",
    "             \"284bf2dd-c310-4a21-8a4b-eedc20fea675\",\n",
    "             \"John Doe\",\n",
    "             \"2022-03-24 21:00:24.801188\",\n",
    "             \"2022-03-24 21:00:24.801194\",\n",
    "             \"domain\")]\n",
    "\n",
    "columns = [\"Op\", \n",
    "           \"sys_change_timestamp\",\n",
    "           \"id\",\n",
    "           \"name\",\n",
    "           \"created_at\",\n",
    "           \"updated_at\",\n",
    "           \"sys_data_source\"]\n",
    "\n",
    "# create spark dataframe\n",
    "df = spark_session.createDataFrame(values, columns)    \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256876c",
   "metadata": {},
   "source": [
    "## Write csv files into the s3\n",
    "Now let's write the data frame created above into our s3 bucket in csv format. After this step your first task will start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4f5d0-ec8a-4dd6-ab7d-5fb1ee206a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csv files into the s3\n",
    "def write_to_s3(s3_source):    \n",
    "    for path in s3_source:    \n",
    "        df.write.option(\"header\",True).csv(path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3026ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to s3 for each path in the list\n",
    "write_to_s3(s3_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e81bf4",
   "metadata": {},
   "source": [
    "## Task 1 - S3 Data Verification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077afadd",
   "metadata": {},
   "source": [
    "Now is the time for your first task. In the below function \"get_s3_object()\" you have to complete the code to retrieve all objects for the s3 bucket we created and saved previously in CSV format files. You have to use AWS SDK for Python (Boto3) to access AWS s3 services and store them in the variable \"s3_client\" that we created below. Use this variable to get all objects in the s3 bucket and return as \"respond\". Basically, your task is to return the s3 respond to be used in the next cell to list all objects. Please note that \"s3_client\" is complete and you just need to use it to get the information from the s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954ef3c-96b1-49f9-b614-144da06464ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def get_s3_object():\n",
    "    \n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION, endpoint_url=S3_MOCK_ENDPOINT)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError(\"Your code should be implement and delete this line of code\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d84e7",
   "metadata": {},
   "source": [
    "Now let's list all obejets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63244786",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_s3_object()\n",
    "\n",
    "# Output the bucket names\n",
    "files = response.get(\"Contents\")\n",
    "for file in files:\n",
    "    print(f\"file_name: {file['Key']}, size: {file['Size']}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28205c",
   "metadata": {},
   "source": [
    "## Unit Tests\n",
    "This unit test will help you to know if your answers have been submitted correctly. If you are familiarized with **assert** in python you know that if you don't get an exception with the assert message that means your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d78dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_s3_object().get(\"Contents\") != None, \"Incorrect return value: There are not object for the s3 bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb954a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(get_s3_object(), dict) == True, \"Incorrect return value: Function should return a dictionary\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daaffd6",
   "metadata": {},
   "source": [
    "## Read data\n",
    "Now let's prepare for task 2. The below cell will load the s3 data into a spark data frame that you will use in task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b12540-5bc5-4d61-8184-d1f13ecab645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.read.format(\"csv\") \\\n",
    "      .option(\"recursiveFileLookup\",\"true\") \\\n",
    "      .option(\"header\", \"true\") \\\n",
    "      .option(\"sep\", \",\") \\\n",
    "      .load(\"s3a://{}/{}\".format(BUCKET_NAME, \"data/raw-zone/domain\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7b31f",
   "metadata": {},
   "source": [
    "## Task 2 - Add Partitions\n",
    "You have to add to the data frame created above 3 new columns: year, month, and day in addition to the existing ones. You will get the values or content of these 3 new columns in the s3 partition folder in the format of \"yyyy/mm/dd\". For example in the path \"data/raw-zone/domain/2022/03/23/\", the 2022/03/23 is the value for your new columns in the data frame. You will use as value 2022 to create the column year, 03 for the column month, and 23 for the column day. Example of the result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a311cba",
   "metadata": {},
   "source": [
    "year |month |day\n",
    "----|----|---- \n",
    "2022|03|23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b0b3b-4ad0-4a70-a683-cf8233b11648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def map_function(x):\n",
    "    # create colunms form path format: year/month/day\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Your code should be implement and delete this line of code\")\n",
    "    \n",
    "    return Op,sys_change_timestamp,id,name,created_at,updated_at,sys_data_source,filename,year, month, day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3cd213",
   "metadata": {},
   "source": [
    "The code below will use your answer in the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfe218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add file name and path \n",
    "df_fn = df.withColumn(\"filename\", input_file_name())\n",
    "\n",
    "# map using the map_function\n",
    "df_partions = df_fn.rdd.map(lambda x: map_function(x)).toDF \\\n",
    "([\"Op\",\"sys_change_timestamp\",\"id\",\"name\",\"created_at\",\"updated_at\",\"sys_data_source\",\"filename\",\"year\",\"month\",\"day\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2646bd",
   "metadata": {},
   "source": [
    "## Unit Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_partions.schema.simpleString().find(\"day\") > 0, \"Incorrect Return Value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814af27a-e05f-4ef2-b812-fb1320b9ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_partions.schema.simpleString().find(\"month\") > 0, \"Incorrect Return Value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062891d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_partions.schema.simpleString().find(\"year\") > 0, \"Incorrect Return Value: Value obtained does not match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede28265",
   "metadata": {},
   "source": [
    "## Task 3 - Write Data to S3 Partitioned In Parquet Format\n",
    "This is the last task. Here you have to complete the below function to write the data that you created using the folder partitions into an s3 bucket in parquet format. This data need to be saved partitioned by \"year\", \"month\", and \"day\" using spark. Example: year=2022/month=03/day=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f46d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Insert your answer in this cell. DO NOT CHANGE THE NAME OF THE FUNCTION.\n",
    "def write_to_s3_parquet(data):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError(\"Your code should be implement and delete this line of code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_s3_parquet(df_partions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = spark_session.sql(\"select * from parquet.`{}year=2022/month=03/day=23`\".format(s3_destination))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71483a",
   "metadata": {},
   "source": [
    "## Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54615334",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (not df_q.rdd.isEmpty() == True), \"The dataframe is empty\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
